{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "spec_morph_filter.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyOJy0pAeQEEgI+wu455YjTP",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/joeljose/audio_denoising/blob/main/spec_morph_filter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-hQPeQwLCGY0"
   },
   "source": [
    "#Noise Compensation using Spectrogram Morphological Filtering\n",
    "In this project we try out image based morphological filtering to audio spectrograms for removing noise from audio signals. Regions of the spectrogram having high energy are estimated to be of more importance and likely to contain the original audio signal. The process of erosion can remove noise while dilation can then restore any erroneously removed signal regions. The combination of the two techniques results\n",
    "in a non-linear, time-frequency filter. We then recreate the denoised audio from the processed spectrogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Pc5-vaRsIZj"
   },
   "source": [
    "## Importing all the essential modules:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6xbPX_osPYyG"
   },
   "source": "from scipy.ndimage import binary_erosion\nfrom scipy.ndimage import binary_dilation\nimport scipy.signal as signal\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Audio\nimport librosa",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GxGMfT6FrPd"
   },
   "source": "## Creating a noisy audio signal\nFirst off let's create an audio signal with 5 musical tones.\n "
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HpDopRBDwg7x"
   },
   "source": "np.random.seed(42)\n\nfs=10000                                          # sampling frequency in Hz\nnotes=[837.31,1939.85,1054.94,1939.85,837.31]     # note frequencies in Hz\nnote_interval= 1                                  # duration of each note in seconds\nsong_time=len(notes)*note_interval                # Total duration of the song\n\n\ndt=1/fs\nt=np.arange(0,note_interval,dt)\n\ntones=[]\n\nfor fund_freq in notes:\n  tones.append(np.sin(2 * np.pi * fund_freq * t)) # each note is a sine wave \n\nsong = np.concatenate(tones)                      # add up all the notes to get our song",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-kULSzZb7x3x"
   },
   "source": [
    "Now let's add some noise to the song we created"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q3BfRwIcGnCw"
   },
   "source": "# creating a noise with the same dimension as input signal \n\nmu, sigma = 0, 1                                  # mean and standard deviation of noise signal\nnoise = np.random.normal(mu, sigma, song.shape)  # generate noise signal using random function\n\nnoisy_song = song + noise                        # add the noise to the song to get noisy song",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOb0H6Ai6Veq"
   },
   "source": [
    "Let's hear the song"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s0aXyjZaIOIM"
   },
   "source": [
    "Audio(song, rate=fs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb4pfODh6U28"
   },
   "source": [
    "Now let's hear the noisy signal "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HAgrtuVpN3yx"
   },
   "source": [
    "Audio(noisy_song, rate=fs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOvBhht9sP0S"
   },
   "source": [
    "##helper functions for creating plots and displaying them"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dmZf5a3b0rL1"
   },
   "source": "def draw_spectrogram(time, freq, magnitude, title, cmap='inferno'):\n  \"\"\"Plot a spectrogram from the magnitude of the STFT.\"\"\"\n  fig, ax = plt.subplots(figsize=(10, 5))\n  mesh = ax.pcolormesh(time, freq, magnitude, shading='gouraud',\n                       cmap=plt.get_cmap(cmap))\n  ax.set_ylim([freq[1], freq[-1]])\n  ax.set_title('Spectrogram of ' + title)\n  ax.set_ylabel('Frequency [Hz]')\n  ax.set_xlabel('Time [sec]')\n  fig.colorbar(mesh, ax=ax)\n\ndef draw_timeseries(input_signal, time):\n  \"\"\"Plot an audio signal waveform.\"\"\"\n  fig, ax = plt.subplots(figsize=(10, 5))\n  ax.plot(time, input_signal, color='c', linewidth=1.5, label=\"input\")\n  ax.set_xlim(time[0], time[-1])\n  ax.set_title('Signal')\n  ax.set_ylabel('Amplitude')\n  ax.set_xlabel('Time [sec]')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aGBqtdT56m0x"
   },
   "source": [
    "## Spectrograms of the song and noisy song we created.\n",
    "We can create spectrograms of the audio signal by taking the STFT(short time fourier transform) of the signal and plotting just the magnitude of this complex valued function(Zxx)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2y1qN9_EJZuM"
   },
   "source": [
    "nperseg = 512                                             # no. of samples in a segment\n",
    "\n",
    "freq, ti, Zxx = signal.stft(song, fs=fs, nperseg=nperseg) #STFT function\n",
    "\n",
    "draw_spectrogram(ti,freq,np.abs(Zxx),'song')              # we use our helper function to draw the spectrogram"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M3NMJ3PoN-Ef"
   },
   "source": [
    "nperseg = 512\n",
    "\n",
    "freq, ti, Zxx = signal.stft(noisy_song, fs=fs, nperseg=nperseg)\n",
    "\n",
    "draw_spectrogram(ti,freq,np.abs(Zxx),'noisy song')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33MwiCbW60TM"
   },
   "source": [
    "Zxx is a complex valued signal(stft of the audio signal) whose magnitude we plot as the spectrogram.\n",
    "By comparing the spectrograms of the original and noisy audio signals, we see that if we can filter out just the bright parts, we can reconstruct the original without any of the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S18HG-PmDRYu"
   },
   "source": [
    "## Denoise function\n",
    "We need to make a function that takes in the complex valued Zxx and gives out the complex valued Rxx as the recovered stft signal, with which we can then do inverse stft to get our final recovered denoised audio signal.\n",
    "\n",
    "There are 4 steps in the process:\n",
    "* Create a grayscale image from the magnitude of Zxx.\n",
    "* Apply binary thresholding.\n",
    "* Apply morphological processing to get the mask image.\n",
    "* Using this mask image we create Rxx."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_ezXrSjqnxFn"
   },
   "source": "def morph_denoise(Zxx, threshold, amp, plot=True):\n  \"\"\"Apply morphological denoising to a complex-valued STFT matrix.\n\n  Parameters\n  ----------\n  Zxx : ndarray\n      Complex STFT matrix.\n  threshold : float\n      Binary threshold applied to the grayscale spectrogram (0-255).\n  amp : float\n      Amplification/attenuation factor for masked/unmasked regions.\n  plot : bool\n      If True, display intermediate processing images.\n\n  Returns\n  -------\n  Rxx : ndarray\n      Denoised complex STFT matrix.\n  \"\"\"\n  zmax = np.max(np.abs(Zxx))\n  gray_image = np.abs(Zxx) * (255 / zmax)\n\n  thresh_image = np.where(gray_image >= threshold, 1, 0)  # binary thresholding\n\n  mask_image = binary_erosion(thresh_image, iterations=1)\n  mask_image = binary_dilation(mask_image, iterations=2)\n\n  if plot:\n    fig, axis_arr = plt.subplots(1, 3, figsize=(15, 5))\n    axis_arr[0].set_title(\"grayscale spectrogram\")\n    axis_arr[0].imshow(gray_image, cmap=plt.cm.gray)\n    axis_arr[1].set_title(\"After binary thresholding\")\n    axis_arr[1].imshow(thresh_image, cmap=plt.cm.gray)\n    axis_arr[2].set_title(\"After Morphological filtering\")\n    axis_arr[2].imshow(mask_image, cmap=plt.cm.gray)\n    plt.tight_layout()\n\n  Rxx = np.where(mask_image == 1, Zxx * amp, Zxx / amp)\n\n  return Rxx",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNHRCPbHkgJV"
   },
   "source": [
    "Now let's apply the denoise function on Zxx to get Rxx."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y49ySITbezbs"
   },
   "source": [
    "Rxx=morph_denoise(Zxx,threshold=40,amp=10)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJrSYWIybIEz"
   },
   "source": [
    "The image we process looks different from the spectrogram because the axis are flipped(image indices start from 0 from the top). But that doesn't affect the processing.\n",
    " \n",
    "Let's take a look at the spectrogram of the recovered signal."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ncYGjXUuff--"
   },
   "source": [
    "draw_spectrogram(ti,freq,np.abs(Rxx),'recovered song')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HULimF_HWwI8"
   },
   "source": [
    "Now let's reconstruct the audio signal from Rxx by applying inverse short time fourier transform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1kZuWpsLfcbD"
   },
   "source": [
    "_, xrec = signal.istft(Rxx, fs)\n",
    "\n",
    "Audio(xrec, rate=fs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMQrZhXVxRR5"
   },
   "source": [
    "## Applications\n",
    "This project was part of a bigger project called \"Visual Mic\", where we extract audio from motion due to vibrations caused by sound waves on materials.\n",
    "Here is the link to the project: https://github.com/joeljose/Visual-Mic </br>  This denoising algorithm was effective in extracting out the signal from the noisy audio. First let me show you the audio which we got from visual mic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it2iV5_Z4RL-"
   },
   "source": [
    "Let's get the audio file from my github repository "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ajTKWsBp86W8"
   },
   "source": "import os\nimport urllib.request\n\nwav_url = \"https://github.com/joeljose/assets/raw/master/audio_denoising/visualmic.wav\"\nwav_path = \"visualmic.wav\"\n\nif not os.path.exists(wav_path):\n    print(\"Downloading visualmic.wav ...\")\n    urllib.request.urlretrieve(wav_url, wav_path)\n    print(\"Done.\")\nelse:\n    print(\"visualmic.wav already exists, skipping download.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W1JzDUWw0Adz"
   },
   "source": [
    "Now lets load the audio file, print details about it and also plot the audio waveform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EnS7e2GclOTv"
   },
   "source": "noisy_signal, fs_wav = librosa.load(\"visualmic.wav\", sr=None)\n\nn = len(noisy_signal)\ndt = 1 / fs_wav\ntot_time = np.floor(n * dt)\nprint(f'Total playback time :{tot_time} seconds')\nprint(f'Total no. of samples :{n}')\nprint(f'Sampling frequency :{fs_wav} Hz')\n\n\nt = np.arange(0, tot_time, dt)\n\n# Plot the signal \n\ndraw_timeseries(noisy_signal, t)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dciND1G2KzO3"
   },
   "source": [
    "Let's hear it"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q6NTnVVmK4Hv"
   },
   "source": "Audio(noisy_signal, rate=fs_wav)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxcgC057LDCi"
   },
   "source": [
    "Let's also look at it's spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cOmWfHsqKR18"
   },
   "source": "nperseg = 512\n\nfreq, ti, Zxx = signal.stft(noisy_signal, fs=fs_wav, nperseg=nperseg)\n\ndraw_spectrogram(ti, freq, np.abs(Zxx), 'noisy signal obtained from visual mic')",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVUGU7FWLVt5"
   },
   "source": [
    "Now let's apply the denoise function on Zxx to get Rxx."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-Nv5StCULVt7"
   },
   "source": [
    "Rxx=morph_denoise(Zxx,threshold=20,amp=10)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEJmknOdLmRw"
   },
   "source": [
    "Let's take a look at the spectrogram of the recovered signal."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "s8jR9qp1LmR2"
   },
   "source": [
    "draw_spectrogram(ti,freq,np.abs(Rxx),'recovered song')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o_G_xj9LmR6"
   },
   "source": [
    "Now let's reconstruct the audio signal from Rxx by applying inverse short time fourier transform."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZvuzM-CWLmR-"
   },
   "source": "_, xrec = signal.istft(Rxx, fs_wav)\n\nAudio(xrec, rate=fs_wav)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWwE3GNl3jRP"
   },
   "source": [
    "The reconstructed audio is a much more clear audio of \"mary had a little lamb\".\n"
   ]
  }
 ]
}